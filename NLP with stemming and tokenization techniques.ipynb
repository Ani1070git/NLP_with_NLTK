{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896e1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminologies of Nlp\n",
    "#--------------------------#\n",
    "# Corpus - Paragraph of a given data\n",
    "#Documents - Sentence of given data\n",
    "#Vocabulary - Unique words\n",
    "#words - which are present in a corpus.\n",
    "\n",
    "#Tokenization\n",
    "#-----------------------#\n",
    "#It is the process of converting a sentence or a corpus into individual tokens.\n",
    "#corpus(set of sentences) -> single sentences -> tokens(ehich contains unique words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17423cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is required to use nltk's sent_tokenize.\n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb82120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81811282",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"hello welcome , i am watching krish naiks NLP tutorials in python.\n",
    "By watching the entire course! i can become a expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84914f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome , i am watching krish naiks NLP tutorials in python.\n",
      "By watching the entire course! i can become a expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1f940",
   "metadata": {},
   "source": [
    "# Tokenization and its types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd96e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toeknization\n",
    "#performing tokenization to convert paragraphs into -> sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b77255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the paragrapgh into the sent_tokenize to convert into individual sentences\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d9d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello welcome , i am watching krish naiks NLP tutorials in python.', 'By watching the entire course!', 'i can become a expert in NLP.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d140bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding the type of data stored.\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2e5675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome , i am watching krish naiks NLP tutorials in python.\n",
      "By watching the entire course!\n",
      "i can become a expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "#just iterating through the list \n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2cd3c0",
   "metadata": {},
   "source": [
    "# 1.Word Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e10ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In next Tokenization technique ,\n",
    "#we are converting sentences into -> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73926e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'watching',\n",
       " 'krish',\n",
       " 'naiks',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " 'in',\n",
       " 'python',\n",
       " '.',\n",
       " 'By',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'i',\n",
       " 'can',\n",
       " 'become',\n",
       " 'a',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing the corpus to get the sentence converted into words\n",
    "word_tokenize(corpus)\n",
    "#this is performed to know the importance of each and every words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548a2b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'welcome', ',', 'i', 'am', 'watching', 'krish', 'naiks', 'NLP', 'tutorials', 'in', 'python', '.']\n",
      "['By', 'watching', 'the', 'entire', 'course', '!']\n",
      "['i', 'can', 'become', 'a', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#once again iterating through sentence\n",
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))\n",
    "#we see the separation of sentence into words.Then we can perform cleaning and other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cb3bd",
   "metadata": {},
   "source": [
    "# 2.Word Punct Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605b7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have puntuations in a sentence we separate it as a token\n",
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e09a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'watching',\n",
       " 'krish',\n",
       " 'naiks',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " 'in',\n",
       " 'python',\n",
       " '.',\n",
       " 'By',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'i',\n",
       " 'can',\n",
       " 'become',\n",
       " 'a',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)\n",
    "#As of now this corpus does not contains any punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad695a",
   "metadata": {},
   "source": [
    "# 3. TreebankWordTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e22595e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we use tree bank tokenize which does not \n",
    "#treat punctuations and full stops as separate tokens\n",
    "#except the last full stop of the sentence\n",
    "\n",
    "from nltk import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606a7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TreebankWordTokenize this does not takes any arguments\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8af244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'watching',\n",
       " 'krish',\n",
       " 'naiks',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " 'in',\n",
       " 'python.',\n",
       " 'By',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'i',\n",
       " 'can',\n",
       " 'become',\n",
       " 'a',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)\n",
    "#we observe the difference as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "413e6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is all about tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc84b2",
   "metadata": {},
   "source": [
    "# Stemming and its types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2af83a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next is Stemming \n",
    "#Stemming is mainly used in cases like wether the email is spam or not.\n",
    "#This is a process in which the words are reduced to it stem word\n",
    "#Example -> [eaten,eating,eats] -> but stem_word is -> eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba170b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d3ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First stemming method we will be using is porter stemming or porter stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aa713",
   "metadata": {},
   "source": [
    "# 1. PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20ecdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the stemmer\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3347a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storin it in a variable\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac3f6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---------->eat\n",
      "eats---------->eat\n",
      "eaten---------->eaten\n",
      "writing---------->write\n",
      "writes---------->write\n",
      "programming---------->program\n",
      "programs---------->program\n",
      "history---------->histori\n",
      "finally---------->final\n",
      "finalized---------->final\n"
     ]
    }
   ],
   "source": [
    "#to apply stemming we will iterate over the words to get stem word\n",
    "for word in words:\n",
    "    print(word + \"---------->\" + stemming.stem(word))\n",
    "#here stem is method where passed words are reduced into their root word.\n",
    "\n",
    "#The major disadvantage is that some of the words lose it meaning when it reduced\n",
    "#Example is -> history---------->histori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4818e",
   "metadata": {},
   "source": [
    "# 2. RegexpStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4186c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regexpstemmer class\n",
    "#Here we can implement regular expression with help of regexp stemmer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93411b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8accc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ing$|s$|e$|able$' -> these are the regular expression and 4 the length that it accepts.\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4) \n",
    "# $ -> this represents that the model will remove the ing from the last or starting.\n",
    "#if it used in the start the ing will be remove from the start if it is in the last it removes from the last\n",
    "#if not mentioned it removes from both start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8ac8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 'ing' in the word will be removed regexp stemmer.\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b09ebad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demostration for the $ sign change.\n",
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1dba9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $ -> this is present in the last so 'ing' of the last is removed\n",
    "reg_stemmer.stem('ingeating') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59edebc",
   "metadata": {},
   "source": [
    "# 3. SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "713c9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snow ball stemmer is another type of stemmer\n",
    "#which is the better than porter stemmer.\n",
    "#This Technique is mainly used text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77fa0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acc002ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can pass different languages and we need to specify it \n",
    "#Here we are using english\n",
    "snowballsstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb227a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>histori\n",
      "finally------>final\n",
      "finalized------>final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"------>\" + snowballsstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a984188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can see the difference between porter stemmer and snow ball stemmer\n",
    "stemming.stem('fairly') , stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f14c31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#But the result are better in snowball stemmer\n",
    "snowballsstemmer.stem('fairly') ,snowballsstemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547825e7",
   "metadata": {},
   "source": [
    "# Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e323e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To solve all the problems and limitations we use lemmatization\n",
    "#Which is mainly used in chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization is technique like stemming.\n",
    "#The output after lemmatization we get is called lemma.\n",
    "#The lemma is root word rather than root stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "046e970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49080944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a83e9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing with variable lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf02997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here the going word is considered a root word and it considered as noun form.\n",
    "lemmatizer.lemmatize(\"going\" , pos = 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1473d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parts of speech tag is used. #Pos is called post tag.\n",
    "'''\n",
    "Pos - Noun - n\n",
    "      verb - v\n",
    "      adjective - a\n",
    "      adverb - r    \n",
    "'''\n",
    "lemmatizer.lemmatize(\"going\" , pos = 'v') \n",
    "#Part of speech used here is verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db808ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can see the power of lemmatization.\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "370eb3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eating\n",
      "eats------>eats\n",
      "eaten------>eaten\n",
      "writing------>writing\n",
      "writes------>writes\n",
      "programming------>programming\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalized\n"
     ]
    }
   ],
   "source": [
    "#Here pos tag has noun as argument.\n",
    "for word in words:\n",
    "    print(word + \"------>\" + lemmatizer.lemmatize(word)) #All the output are correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f625be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalize\n"
     ]
    }
   ],
   "source": [
    "#Here pos tag has verb as argument.\n",
    "for word in words:\n",
    "    print(word + \"------>\" + lemmatizer.lemmatize(word , pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applications or usage of lemmatization are \n",
    "#Q and A  #Chatbots  #Text summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
